/**
 * robots.txt generator module
 * Creates robots.txt files for search engine crawling control
 */

import type { CodebaseProfile } from '../types';
import { getHandler } from '../scanner/frameworks';

/**
 * Configuration for robots.txt generation
 */
export interface RobotsConfig {
  /** Base domain URL */
  domain: string;
  /** Paths to disallow for all crawlers */
  disallowPaths: string[];
  /** Paths to specifically allow */
  allowPaths: string[];
  /** Crawl delay in seconds (optional) */
  crawlDelay?: number;
  /** Custom rules for specific user agents */
  customRules: Map<string, { allow: string[]; disallow: string[] }>;
  /** Whether to include sitemap reference */
  includeSitemap: boolean;
  /** Custom sitemap URL (defaults to /sitemap.xml) */
  sitemapUrl?: string;
}

const DEFAULT_CONFIG: Omit<RobotsConfig, 'domain'> = {
  disallowPaths: [
    '/api/',
    '/admin/',
    '/_next/',
    '/.well-known/',
    '/private/',
  ],
  allowPaths: [],
  customRules: new Map(),
  includeSitemap: true,
};

/**
 * Generates robots.txt content
 */
export class RobotsGenerator {
  private config: RobotsConfig;

  constructor(domain: string, config: Partial<Omit<RobotsConfig, 'domain'>> = {}) {
    this.config = {
      domain: domain.replace(/\/$/, ''),
      ...DEFAULT_CONFIG,
      ...config,
      customRules: config.customRules || DEFAULT_CONFIG.customRules,
    };
  }

  /**
   * Generate robots.txt content
   *
   * @param disallowPaths - Additional paths to disallow
   * @returns robots.txt content string
   */
  generate(disallowPaths: string[] = []): string {
    const lines: string[] = [];

    // Combine configured and additional disallow paths
    const allDisallowPaths = [...new Set([
      ...this.config.disallowPaths,
      ...disallowPaths,
    ])];

    // Default rules for all user agents
    lines.push('# robots.txt generated by AGI SEO Optimizer');
    lines.push('');
    lines.push('User-agent: *');

    // Allow paths (more specific rules should come first)
    for (const path of this.config.allowPaths) {
      lines.push(`Allow: ${path}`);
    }

    // Disallow paths
    for (const path of allDisallowPaths) {
      lines.push(`Disallow: ${path}`);
    }

    // Add allow for root if we have disallows
    if (allDisallowPaths.length > 0) {
      lines.push('Allow: /');
    }

    // Crawl delay
    if (this.config.crawlDelay) {
      lines.push(`Crawl-delay: ${this.config.crawlDelay}`);
    }

    // Custom rules for specific user agents
    for (const [userAgent, rules] of this.config.customRules) {
      lines.push('');
      lines.push(`User-agent: ${userAgent}`);

      for (const path of rules.allow) {
        lines.push(`Allow: ${path}`);
      }

      for (const path of rules.disallow) {
        lines.push(`Disallow: ${path}`);
      }
    }

    // Sitemap reference
    if (this.config.includeSitemap) {
      lines.push('');
      const sitemapUrl = this.config.sitemapUrl || `${this.config.domain}/sitemap.xml`;
      lines.push(`Sitemap: ${sitemapUrl}`);
    }

    lines.push('');

    return lines.join('\n');
  }

  /**
   * Generate framework-specific robots code
   *
   * @param profile - Codebase profile
   * @returns Generated code string
   */
  generateCode(profile: CodebaseProfile): string {
    const handler = getHandler(profile.framework);
    return handler.generateRobotsCode(this.config.domain);
  }

  /**
   * Get the file path where robots.txt should be saved
   *
   * @param profile - Codebase profile
   * @returns File path
   */
  getRobotsPath(profile: CodebaseProfile): string {
    const handler = getHandler(profile.framework);
    return handler.getRobotsPath();
  }

  /**
   * Add rules for blocking AI crawlers
   *
   * @returns Updated generator with AI crawler rules
   */
  withAIBlockRules(): RobotsGenerator {
    const aiCrawlers = [
      'GPTBot',
      'ChatGPT-User',
      'Google-Extended',
      'CCBot',
      'anthropic-ai',
      'ClaudeBot',
      'Omgilibot',
      'FacebookBot',
    ];

    const newCustomRules = new Map(this.config.customRules);

    for (const crawler of aiCrawlers) {
      newCustomRules.set(crawler, {
        allow: [],
        disallow: ['/'],
      });
    }

    return new RobotsGenerator(this.config.domain, {
      ...this.config,
      customRules: newCustomRules,
    });
  }

  /**
   * Add rules for SEO-specific crawlers
   *
   * @returns Updated generator with SEO crawler rules
   */
  withSEOCrawlerRules(): RobotsGenerator {
    const seoCrawlers = [
      'Googlebot',
      'Bingbot',
      'Slurp', // Yahoo
      'DuckDuckBot',
      'Baiduspider',
      'YandexBot',
    ];

    const newCustomRules = new Map(this.config.customRules);

    for (const crawler of seoCrawlers) {
      newCustomRules.set(crawler, {
        allow: ['/'],
        disallow: [...this.config.disallowPaths],
      });
    }

    return new RobotsGenerator(this.config.domain, {
      ...this.config,
      customRules: newCustomRules,
    });
  }

  /**
   * Validate robots.txt syntax
   *
   * @param content - robots.txt content to validate
   * @returns Validation result with any issues
   */
  static validate(content: string): {
    valid: boolean;
    errors: string[];
    warnings: string[];
  } {
    const errors: string[] = [];
    const warnings: string[] = [];
    const lines = content.split('\n');

    let hasUserAgent = false;
    let currentUserAgent: string | null = null;

    for (let i = 0; i < lines.length; i++) {
      const rawLine = lines[i];
      if (rawLine === undefined) continue;
      const line = rawLine.trim();
      const lineNum = i + 1;

      // Skip empty lines and comments
      if (!line || line.startsWith('#')) {
        continue;
      }

      // Parse directive
      const colonIndex = line.indexOf(':');
      if (colonIndex === -1) {
        errors.push(`Line ${lineNum}: Invalid syntax, missing colon`);
        continue;
      }

      const directive = line.slice(0, colonIndex).toLowerCase().trim();
      const value = line.slice(colonIndex + 1).trim();

      switch (directive) {
        case 'user-agent':
          hasUserAgent = true;
          currentUserAgent = value;
          if (!value) {
            errors.push(`Line ${lineNum}: Empty user-agent value`);
          }
          break;

        case 'allow':
        case 'disallow':
          if (!currentUserAgent) {
            errors.push(`Line ${lineNum}: ${directive} without preceding User-agent`);
          }
          if (value && !value.startsWith('/')) {
            warnings.push(`Line ${lineNum}: Path should start with /`);
          }
          break;

        case 'sitemap':
          if (!value.startsWith('http')) {
            warnings.push(`Line ${lineNum}: Sitemap should be a full URL`);
          }
          break;

        case 'crawl-delay':
          if (isNaN(Number(value))) {
            errors.push(`Line ${lineNum}: Crawl-delay should be a number`);
          }
          break;

        case 'host':
          // Legacy directive, warn but don't error
          warnings.push(`Line ${lineNum}: Host directive is deprecated`);
          break;

        default:
          warnings.push(`Line ${lineNum}: Unknown directive "${directive}"`);
      }
    }

    if (!hasUserAgent) {
      errors.push('No User-agent directive found');
    }

    return {
      valid: errors.length === 0,
      errors,
      warnings,
    };
  }

  /**
   * Parse existing robots.txt content
   *
   * @param content - robots.txt content
   * @returns Parsed rules
   */
  static parse(content: string): Map<string, { allow: string[]; disallow: string[] }> {
    const rules = new Map<string, { allow: string[]; disallow: string[] }>();
    const lines = content.split('\n');
    let currentUserAgent: string | null = null;

    for (const line of lines) {
      const trimmed = line.trim();

      // Skip empty lines and comments
      if (!trimmed || trimmed.startsWith('#')) {
        continue;
      }

      const colonIndex = trimmed.indexOf(':');
      if (colonIndex === -1) continue;

      const directive = trimmed.slice(0, colonIndex).toLowerCase().trim();
      const value = trimmed.slice(colonIndex + 1).trim();

      switch (directive) {
        case 'user-agent':
          currentUserAgent = value;
          if (!rules.has(value)) {
            rules.set(value, { allow: [], disallow: [] });
          }
          break;

        case 'allow':
          if (currentUserAgent && value) {
            const allowRules = rules.get(currentUserAgent);
            if (allowRules) {
              allowRules.allow.push(value);
            }
          }
          break;

        case 'disallow':
          if (currentUserAgent && value) {
            const disallowRules = rules.get(currentUserAgent);
            if (disallowRules) {
              disallowRules.disallow.push(value);
            }
          }
          break;
      }
    }

    return rules;
  }
}

/**
 * Convenience function to generate robots.txt
 */
export function generateRobots(domain: string, disallowPaths: string[] = []): string {
  const generator = new RobotsGenerator(domain);
  return generator.generate(disallowPaths);
}

/**
 * Convenience function to generate robots.txt with AI blockers
 */
export function generateRobotsWithAIBlock(
  domain: string,
  disallowPaths: string[] = []
): string {
  const generator = new RobotsGenerator(domain).withAIBlockRules();
  return generator.generate(disallowPaths);
}
